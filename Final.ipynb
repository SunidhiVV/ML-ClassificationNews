{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ML Final Project\n"
      ],
      "metadata": {
        "id": "3w8DdXc3zdXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries\n"
      ],
      "metadata": {
        "id": "kJf3HbnhzeEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhu5XTkAv9zn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "ZedwPRGyzJ0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnTRr01nzLst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset"
      ],
      "metadata": {
        "id": "9thZxUbiznmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Final ML/22204097.csv')"
      ],
      "metadata": {
        "id": "z_mDk-QvwOW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define our function with which we shall preprocess and tokenize our text\n"
      ],
      "metadata": {
        "id": "33HPjpUdzp_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_tokenize(data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(data)\n",
        "    filtered_text = [word for word in word_tokens if word.casefold() not in stop_words]\n",
        "    return filtered_text"
      ],
      "metadata": {
        "id": "slCH7wlgxFXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze the most commonly occuring words for each category"
      ],
      "metadata": {
        "id": "PRB2ugDR0QEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def analyze():\n",
        "  for category in df['category'].unique():\n",
        "      words = []\n",
        "      for row in df[df['category'] == category][['headline', 'short_description']].values:\n",
        "          words.extend(preprocess_and_tokenize(\" \".join(row)))\n",
        "      most_common_terms = Counter(words).most_common(10)\n",
        "      print(f\"For {category}, the most common terms are: {most_common_terms}\")\n",
        "#analyze()\n"
      ],
      "metadata": {
        "id": "AGYAVdJE0T9X"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From our output, it seems that there are missing values which requires us to clean our data.\n"
      ],
      "metadata": {
        "id": "GNdb_ry1zzQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['headline'] = df['headline'].fillna('')\n",
        "df['short_description'] = df['short_description'].fillna('')\n"
      ],
      "metadata": {
        "id": "CUd9NdoFyIeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove any rows with missing text\n"
      ],
      "metadata": {
        "id": "I40YdN-r0F0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['headline', 'short_description'])\n"
      ],
      "metadata": {
        "id": "LHP1_8vWyLfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt to Analyze the most commonly occuring words for each category again"
      ],
      "metadata": {
        "id": "ZDLqC3gQ0n3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyze()"
      ],
      "metadata": {
        "id": "ihbtIKH7xPPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our output doesn't seem to actually align with obtaining the most common words. This seems to be a side effect of tokenization.\n",
        "Let's revise the preprocessing function to exclude punctuation by using 'string.punctuation'."
      ],
      "metadata": {
        "id": "l2O6f1Kizbcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def preprocess_and_tokenize(data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.update(set(string.punctuation))\n",
        "    word_tokens = word_tokenize(data)\n",
        "    filtered_text = [word for word in word_tokens if word.casefold() not in stop_words]\n",
        "    return filtered_text\n"
      ],
      "metadata": {
        "id": "KxRjv5-81BRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rerun the analysis\n"
      ],
      "metadata": {
        "id": "SCCJhrrH1EmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyze()"
      ],
      "metadata": {
        "id": "nYbRz4RY1Ha-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While relatively better, our output is still including unwanted strings such as \"'s\", \"n't\" etc\n",
        "We shall use the 're' module to remove non-alphanumeric characters from our strings. We shall then use 'contractions' library. This is so we can split contractions into separate words."
      ],
      "metadata": {
        "id": "4R-YeLKW1f3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_tokenize(data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.update(set(string.punctuation))\n",
        "    expanded_words = []\n",
        "    for word in data.split():\n",
        "        expanded_words.append(contractions.fix(word))\n",
        "    data = ' '.join(expanded_words)\n",
        "    data = re.sub(r'\\W', ' ', data)\n",
        "    data = re.sub(r'\\s+', ' ', data)\n",
        "    word_tokens = word_tokenize(data)\n",
        "    filtered_text = [word for word in word_tokens if word.casefold() not in stop_words]\n",
        "    return filtered_text"
      ],
      "metadata": {
        "id": "V_YOIM3I11k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of what is going on in this function: \"don't\" is converted to \"do not\".\n",
        "Run analyze() again"
      ],
      "metadata": {
        "id": "VHdtJNqH14l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyze()"
      ],
      "metadata": {
        "id": "nA8aEU3M2GZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output is much more representative of the answer we are looking for: the most occuring words in each category."
      ],
      "metadata": {
        "id": "mXkxtQ0I2FwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a visual representation of our findings\n"
      ],
      "metadata": {
        "id": "VgsOACyI23tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_common_words(category):\n",
        "    words = []\n",
        "    for row in df[df['category'] == category][['headline', 'short_description']].values:\n",
        "        words.extend(preprocess_and_tokenize(\" \".join(row)))\n",
        "    most_common_terms = dict(Counter(words).most_common(10))\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(most_common_terms.keys(), most_common_terms.values())\n",
        "    plt.title(f'Most Common Words in {category}')\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "for category in df['category'].unique():\n",
        "    if pd.isna(category):\n",
        "        continue\n",
        "    plot_common_words(category)\n"
      ],
      "metadata": {
        "id": "dcX-r-853MAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze some other features and their relationships with labels.\n",
        "For starters, we could analyze the length of the headlines and the descriptions for each category."
      ],
      "metadata": {
        "id": "J_-2zYdJ6drB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['headline_length'] = df['headline'].apply(lambda x: len(word_tokenize(x)))\n",
        "df['description_length'] = df['short_description'].apply(lambda x: len(word_tokenize(x)))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(data=df, x='category', y='headline_length')\n",
        "plt.title('Headline length by category')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(data=df, x='category', y='description_length')\n",
        "plt.title('Description length by category')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "20tZOpW_6p46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n"
      ],
      "metadata": {
        "id": "x53pi-Kw7g3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform a date-time analysis to see if we can discover any interesting insights."
      ],
      "metadata": {
        "id": "rlCbC4mR7_yP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trend Analysis to see if there are any noticeable trends."
      ],
      "metadata": {
        "id": "cIxsw0OL8TXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.groupby([df.date.dt.year, 'category']).size().unstack().plot(kind='line', subplots=True)\n"
      ],
      "metadata": {
        "id": "HfKo90x37kht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seasonality Analysis to see if we can look for patterns that recur at regular intervals."
      ],
      "metadata": {
        "id": "OZdDINO28Y6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby([df.date.dt.month, 'category']).size().unstack().plot(kind='line', subplots=True)\n"
      ],
      "metadata": {
        "id": "eJg0vnFp8eb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**\n"
      ],
      "metadata": {
        "id": "inyLJqCM7jZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into training, development and test sets.\n",
        "We shall be looking to split our data into 60% for training, 20% for validation and 20% for testing."
      ],
      "metadata": {
        "id": "kPnDGdwL9cHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "# Split into training and temp\n",
        "df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42, stratify=df['category'])\n",
        "\n",
        "# Split temp into validation and test\n",
        "df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp['category'])\n",
        "\n",
        "# Save the datasets into csv files\n",
        "df_train.to_csv('train.csv', index=False)\n",
        "df_valid.to_csv('valid.csv', index=False)\n",
        "df_test.to_csv('test.csv', index=False)\n"
      ],
      "metadata": {
        "id": "KGlVTV5p9qsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = pd.read_csv('test.csv')\n",
        "testes.head()"
      ],
      "metadata": {
        "id": "W5gl5xKBEEEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might need to remove NaN values."
      ],
      "metadata": {
        "id": "TgdgU9ZF98ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['category'])\n"
      ],
      "metadata": {
        "id": "p6NQI29p-Cj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and tempe\n",
        "df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42, stratify=df['category'],shuffle=True)\n",
        "\n",
        "# Split temp into valid and test\n",
        "df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp['category'],shuffle=True)\n",
        "\n",
        "# Save the datasets into csv files\n",
        "df_train.to_csv('train.csv', index=False)\n",
        "df_valid.to_csv('valid.csv', index=False)\n",
        "df_test.to_csv('test.csv', index=False)\n"
      ],
      "metadata": {
        "id": "eZO8t9Hd-uxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['headline'] = df_train['headline'].fillna('')\n",
        "df_train['short_description'] = df_train['short_description'].fillna('')\n",
        "\n",
        "df_valid['headline'] = df_valid['headline'].fillna('')\n",
        "df_valid['short_description'] = df_valid['short_description'].fillna('')\n"
      ],
      "metadata": {
        "id": "rDWT_VhE_XW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now load data and apply preprocessing steps"
      ],
      "metadata": {
        "id": "NCUsy1kD-zQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the training and validation datasets\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_valid = pd.read_csv('valid.csv')\n",
        "df_train['headline'] = df_train['headline'].fillna('')\n",
        "df_train['short_description'] = df_train['short_description'].fillna('')\n",
        "\n",
        "df_valid['headline'] = df_valid['headline'].fillna('')\n",
        "df_valid['short_description'] = df_valid['short_description'].fillna('')\n",
        "\n",
        "'''\n",
        "# Initialize a TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train = vectorizer.fit_transform(df_train['headline'] + ' ' + df_train['short_description'])\n",
        "\n",
        "# Transform the validation data\n",
        "X_valid = vectorizer.transform(df_valid['headline'] + ' ' + df_valid['short_description'])\n",
        "'''\n",
        "# Create a TF-IDF Vectorizer instance\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
        "                             stop_words='english',\n",
        "                             max_df=0.5,\n",
        "                             min_df=2,\n",
        "                             max_features=5000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train = vectorizer.fit_transform(df_train['headline'] + ' ' + df_train['short_description'])\n",
        "\n",
        "# Transform the validation data\n",
        "X_valid = vectorizer.transform(df_valid['headline'] + ' ' + df_valid['short_description'])\n",
        "\n",
        "# Get the labels\n",
        "y_train = df_train['category']\n",
        "y_valid = df_valid['category']\n"
      ],
      "metadata": {
        "id": "qm7bzEsF-4BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = df_test['category']"
      ],
      "metadata": {
        "id": "9x7nU2QoGkcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building binary classification models"
      ],
      "metadata": {
        "id": "xBRWCOzUD6pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Initialize the classifiers\n",
        "clf1 = LogisticRegression(random_state=42)\n",
        "clf2 = LinearSVC(random_state=42)\n",
        "\n",
        "# Fit the classifiers\n",
        "clf1.fit(X_train, y_train)\n",
        "clf2.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the validation set\n",
        "y_pred1 = clf1.predict(X_valid)\n",
        "y_pred2 = clf2.predict(X_valid)\n"
      ],
      "metadata": {
        "id": "lsVvTG7eD3Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a classifier using deep learning"
      ],
      "metadata": {
        "id": "ZDQS-HTlEFIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "id": "ezJD6hHXG8xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n"
      ],
      "metadata": {
        "id": "VBdbelxlHKzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "id": "VqonnYNrIM80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "id": "wwFjMt7BIQt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# create an encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# fit and transform y_train with the encoder\n",
        "y_train = le.fit_transform(y_train)\n",
        "\n",
        "# transform y_valid with the encoder\n",
        "y_valid = le.transform(y_valid)\n"
      ],
      "metadata": {
        "id": "aUejDbIPPKE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = le.transform(y_test)"
      ],
      "metadata": {
        "id": "ZiTEBSIB5wxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "\n",
        "y_train = y_train.astype(float)\n",
        "y_valid = y_valid.astype(float)\n",
        "\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df_train['headline'] + ' ' + df_train['short_description'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(df_train['headline'] + ' ' + df_train['short_description'])\n",
        "X_valid = tokenizer.texts_to_sequences(df_valid['headline'] + ' ' + df_valid['short_description'])\n",
        "\n",
        "# Pad the sequences\n",
        "X_train = pad_sequences(X_train, maxlen=100)\n",
        "X_valid = pad_sequences(X_valid, maxlen=100)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 64))\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Save the model\n",
        "model.save('text_classifier.h5')\n"
      ],
      "metadata": {
        "id": "tdduEDJPEJNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tokenizer.texts_to_sequences(df_test['headline'] + ' ' + df_test['short_description'])\n",
        "X_test = pad_sequences(X_test, maxlen=100)"
      ],
      "metadata": {
        "id": "TWBr2poeF-vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we need to make sure is to have a clear understanding of the problem we have. It's a binary classification task. There's multiple factors that go into choosing our primary metric. Which mainly depends on the specifics of our problem and the distribution of our classes. Which is why we need to ask the question. Are the classes balanced?"
      ],
      "metadata": {
        "id": "KLtPQ5wAcZmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = df['category'].value_counts()\n",
        "print(class_counts)\n"
      ],
      "metadata": {
        "id": "Y0v_Xs_NWMl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our classes are not balanced! Considering the classes are imbalanced, we want to balance the importance of precision and recall. Thus we shall choose F1-score."
      ],
      "metadata": {
        "id": "zxg-Y-IfYKrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering we have an imbalanced dataset, and the task we have is text classification, we can consider the F1 score in the range of 0.8 to 0.9 to be an appropriate benchmark."
      ],
      "metadata": {
        "id": "UA4tl4IzcUl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train a Logistic Regression model and an SVM model using Scikit-Learn."
      ],
      "metadata": {
        "id": "evaPLqO0gT41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we shall convert our text data into numericals so that our models can be worked on."
      ],
      "metadata": {
        "id": "BaRNMTg5gcE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Combine into single text feature\n",
        "df_train['text'] = df_train['headline'] + ' ' + df_train['short_description']\n",
        "df_valid['text'] = df_valid['headline'] + ' ' + df_valid['short_description']\n",
        "df_test['text'] = df_test['headline'] + ' ' + df_test['short_description']\n",
        "\n",
        "# Initialize a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit the vectorizer and transform the text data\n",
        "X_train_tfidf = vectorizer.fit_transform(df_train['text'])\n",
        "X_valid_tfidf = vectorizer.transform(df_valid['text'])\n",
        "\n",
        "#Test\n",
        "X_test_tfidf = vectorizer.fit_transform(df_test['text'])"
      ],
      "metadata": {
        "id": "z2zI2gUkaCn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now train the logistic regression model\n"
      ],
      "metadata": {
        "id": "JpqX36WygivQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "log_reg.fit(X_train_tfidf, y_train)\n"
      ],
      "metadata": {
        "id": "wUH6HpOMgk_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now train the SVM model\n"
      ],
      "metadata": {
        "id": "AMUT7KPEgqvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm_model = svm.SVC()\n",
        "\n",
        "# Fit the model\n",
        "svm_model.fit(X_train_tfidf, y_train)\n"
      ],
      "metadata": {
        "id": "BlD1pdregsSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save our models using joblib"
      ],
      "metadata": {
        "id": "pGAjTKJagxRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump\n",
        "\n",
        "# Save the models\n",
        "dump(log_reg, 'log_reg.joblib')\n",
        "dump(svm_model, 'svm_model.joblib')\n"
      ],
      "metadata": {
        "id": "N1YOpCO2gw6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load our new models like this."
      ],
      "metadata": {
        "id": "7mn4a8Yrg5tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "\n",
        "\n",
        "model1 = load('log_reg.joblib')\n",
        "model2 = load('svm_model.joblib')\n"
      ],
      "metadata": {
        "id": "B5LseMWBg45G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load our deep learning model\n"
      ],
      "metadata": {
        "id": "73ibcJGCjK72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "dl_model = load_model('text_classifier.h5')\n"
      ],
      "metadata": {
        "id": "4HOdG531jPQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start making our Predictions\n"
      ],
      "metadata": {
        "id": "WVyRXYPsh_dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the TF-IDF data\n",
        "train_preds_model1 = model1.predict(X_train_tfidf)\n",
        "valid_preds_model1 = model1.predict(X_valid_tfidf)\n",
        "\n",
        "train_preds_model2 = model2.predict(X_train_tfidf)\n",
        "valid_preds_model2 = model2.predict(X_valid_tfidf)\n",
        "\n",
        "# For the deep learning model, use the tokenized sequences\n",
        "train_preds_dl_model = (dl_model.predict(X_train) > 0.5).astype(\"int32\")\n",
        "valid_preds_dl_model = (dl_model.predict(X_valid) > 0.5).astype(\"int32\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ES5Az8I3iCNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the TF-IDF data\n",
        "test_preds_model1 = model1.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "test_preds_model2 = model2.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "# For the deep learning model, use the tokenized sequences\n",
        "test_preds_dl_model = (dl_model.predict(X_test) > 0.5).astype(\"int32\")\n"
      ],
      "metadata": {
        "id": "IV3oX1EgGTm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate F1-Score\n"
      ],
      "metadata": {
        "id": "deVoTojhkSs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate F1-score for the train data\n",
        "f1_train_model1 = f1_score(y_train, train_preds_model1)\n",
        "f1_train_model2 = f1_score(y_train, train_preds_model2)\n",
        "f1_train_dl_model = f1_score(y_train, train_preds_dl_model)\n",
        "\n",
        "# Calculate F1-score for the test data\n",
        "f1_test_model1 = f1_score(y_test, test_preds_model1)\n",
        "f1_test_model2 = f1_score(y_test, test_preds_model2)\n",
        "f1_test_dl_model = f1_score(y_test, test_preds_dl_model)\n",
        "\n",
        "# Calculate F1-score for the validation data\n",
        "f1_valid_model1 = f1_score(y_valid, valid_preds_model1)\n",
        "f1_valid_model2 = f1_score(y_valid, valid_preds_model2)\n",
        "f1_valid_dl_model = f1_score(y_valid, valid_preds_dl_model)\n",
        "\n",
        "print(f'Model 1 - F1 Score: Train {f1_train_model1}, Validation {f1_valid_model1}, Test {f1_test_model1}')\n",
        "print(f'Model 2 - F1 Score: Train {f1_train_model2}, Validation {f1_valid_model2}, Test {f1_test_model2}')\n",
        "print(f'Deep Learning Model - F1 Score: Train {f1_train_dl_model}, Validation {f1_valid_dl_model}, Test {f1_test_dl_model}')\n"
      ],
      "metadata": {
        "id": "z5ZFftcXkUN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each model shows a different performance.\n",
        "\n",
        "**Logistic Regression Model**\n",
        "\n",
        "High F1 Score on training set:      **0.9400352733686067**\n",
        "\n",
        "Reduced F1 Score on Validation Set: **0.8789986091794159**\n",
        "\n",
        "**SVM Model**\n",
        "\n",
        "High F1 Score on Training Set: **0.99370012599748**\n",
        "\n",
        "Reduced F1 Score on Validation Set: **0.9093369418132612**\n",
        "\n",
        "**Deep Learning Model**\n",
        "\n",
        "High F1 Score on Training Set: **1.0**\n",
        "\n",
        "Relatively high F1 Score on Validation Set: **0.9145728643216079**\n",
        "\n",
        "Observation\n",
        "\n",
        "Considering how the drop from training to validation is pretty high in SVM and Deep Learning model, it seems to indicate some level of overfitting. Logistic Regression Model is performing relatively well."
      ],
      "metadata": {
        "id": "-k3BYOX7ovuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error Analysis**\n",
        "\n",
        "We need to understand where our models are making mistakes.\n",
        "\n",
        "First, we identify incorrect predictions."
      ],
      "metadata": {
        "id": "gN_k2BpgrjcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are doing a binary classification task, our predictions should be binary as well."
      ],
      "metadata": {
        "id": "UV8fHT_ksCCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatting\n",
        "train_preds_model1_bin = [1 if pred > 0.5 else 0 for pred in train_preds_model1]\n",
        "train_preds_model2_bin = [1 if pred > 0.5 else 0 for pred in train_preds_model2]\n",
        "train_preds_dl_model_bin = [1 if pred > 0.5 else 0 for pred in train_preds_dl_model]\n"
      ],
      "metadata": {
        "id": "GPzyxADgsABX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's find out the incorrect predictions for each model. np.where should help us"
      ],
      "metadata": {
        "id": "k9oSR2tGsH9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incorrect predictions\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "incorrect_preds_model1_indices = np.where(y_valid != valid_preds_model1)\n",
        "incorrect_preds_model2_indices = np.where(y_valid != valid_preds_model2)\n",
        "incorrect_preds_dl_model_indices = np.where(y_valid != valid_preds_dl_model)\n"
      ],
      "metadata": {
        "id": "FhEX8BpRrvAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_valid.shape)\n",
        "print(valid_preds_model1.shape)\n",
        "print(valid_preds_model2.shape)\n",
        "print(valid_preds_dl_model.shape)\n"
      ],
      "metadata": {
        "id": "qBVT6lKNsjS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze common errors. Perhaps there might be a systematic issue with our data or preprocessing steps?"
      ],
      "metadata": {
        "id": "LPnbiFO8t7cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common incorrect predictions\n",
        "common_errors = np.intersect1d(incorrect_preds_model1_indices, np.intersect1d(incorrect_preds_model2_indices, incorrect_preds_dl_model_indices))\n",
        "\n",
        "# Print common errors\n",
        "print('Number of common errors:', len(common_errors))\n"
      ],
      "metadata": {
        "id": "7pRYYM8GuDjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 65 common errors. Let's try to take a closer look and understand the error examples\n"
      ],
      "metadata": {
        "id": "b7XBEUUduwPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some error examples\n",
        "for i in list(common_errors)[:5]:\n",
        "    print(f\"Text: {df_valid.loc[i, 'headline']} {df_valid.loc[i, 'short_description']}\")\n",
        "    print(f\"Actual label: {df_valid.loc[i, 'category']}\")\n",
        "    print(f\"Predicted label (Model 1): {train_preds_model1[i]}\")\n",
        "    print(f\"Predicted label (Model 2): {train_preds_model2[i]}\")\n",
        "    print(f\"Predicted label (Deep Learning Model): {train_preds_dl_model[i]}\\n\")\n"
      ],
      "metadata": {
        "id": "aD2XL_sou2kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "\n",
        "From our error analysis, we seem to find that there have been significant misclassifications. All of them belong to the same 'WORLDPOST' category. Perhaps the errors occur because of geopolitical terms due to how complex the topic is. Or perhaps it is occuring due to cultural contexts. For example, the mention of names like Charlie or Fidel. Our models seem to be struggling with certain types of articles. Therefore, I assume that we would require more training data and that it is not an error in preprocessing."
      ],
      "metadata": {
        "id": "se9TnqscwdFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Calculate the weights for each class\n",
        "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "# Create a dictionary mapping each class to its weight\n",
        "class_weights = dict(enumerate(weights))"
      ],
      "metadata": {
        "id": "9-cyKVNi0a6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest\n",
        "\n",
        "Let's explore a new model\n"
      ],
      "metadata": {
        "id": "Ep-B07lbJ88c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42, n_estimators = 1000, max_depth= 40, max_features ='sqrt')\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "valid_preds_rf_model = rf_model.predict(X_valid_tfidf)\n",
        "\n",
        "rf_model_f1 = f1_score(y_valid, valid_preds_rf_model, average='weighted')\n",
        "print('Random Forest - F1 Score: Validation', rf_model_f1)\n",
        "\n",
        "# Save to file\n",
        "joblib_file = \"rf_model.pkl\"\n",
        "joblib.dump(rf_model, joblib_file)\n"
      ],
      "metadata": {
        "id": "Jz1ouXtZJ8kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load RF"
      ],
      "metadata": {
        "id": "LiJYcEkULF33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "\n",
        "model3 = load('rf_model.pkl')"
      ],
      "metadata": {
        "id": "jNNe9OLnLHNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_model3 = model3.predict(X_test_tfidf)\n",
        "f1_test_model3 = f1_score(y_test, test_preds_model3)\n",
        "f1_test_model3"
      ],
      "metadata": {
        "id": "CWp6WqJBLitq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve our model further, we shall be performing GridSearch."
      ],
      "metadata": {
        "id": "Z48s_F7GMB57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "metadata": {
        "id": "9AJr4sZVMUWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a parameter grid, a dictionary of parameters that helps us in optimization"
      ],
      "metadata": {
        "id": "uS6O6RUVMfTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "63ARTCVpMV2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the GridSearchCV object"
      ],
      "metadata": {
        "id": "4e9xLQdgMqjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='f1_weighted', cv=5, n_jobs=-1)\n"
      ],
      "metadata": {
        "id": "eTCT2w65MYqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit GridSearchCV"
      ],
      "metadata": {
        "id": "XEhIDnwRMw5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.fit(X_train_tfidf, y_train)\n"
      ],
      "metadata": {
        "id": "gQ60N-R_MbWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate so we can see the ebst parameters and the best score"
      ],
      "metadata": {
        "id": "Yi6U5cfmNsiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "b5LXYDvpNsA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the best estimator"
      ],
      "metadata": {
        "id": "XNKuwk-0Ny20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_rf_model = grid_search.best_estimator_\n",
        "valid_preds_best_rf_model = best_rf_model.predict(X_valid_tfidf)\n",
        "test_preds_best_rf_model = best_rf_model.predict(X_test_tfidf)\n",
        "\n",
        "joblib_file = \"best_random_forest.pkl\"\n",
        "joblib.dump(best_rf_model, joblib_file)"
      ],
      "metadata": {
        "id": "muasQaZhN19R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on F1 score"
      ],
      "metadata": {
        "id": "F_8tcTdLOK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_valid_model4 = f1_score(y_valid, valid_preds_best_rf_model, average='weighted')\n",
        "f1_valid_model4"
      ],
      "metadata": {
        "id": "X7RjNTkzOLOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on F1 Score"
      ],
      "metadata": {
        "id": "bu8Ii0L2OceX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_test_model4 = f1_score(y_test, test_preds_best_rf_model)\n",
        "f1_test_model4"
      ],
      "metadata": {
        "id": "1cNeDOfdOcwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shall be running grid search over SVM as well\n"
      ],
      "metadata": {
        "id": "5PGv3hslcTAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import svm\n"
      ],
      "metadata": {
        "id": "mXVG7DpwcTaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'linear', 'poly']\n",
        "}\n"
      ],
      "metadata": {
        "id": "654fTARWc_EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing GridSearchCV"
      ],
      "metadata": {
        "id": "yf1VqefsdAnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc = svm.SVC()\n",
        "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='f1_weighted', cv=5, n_jobs=-1)\n"
      ],
      "metadata": {
        "id": "CDZ_Zz5gdA-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit GridSearchCV"
      ],
      "metadata": {
        "id": "rbOeCXccdCnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.fit(X_train_tfidf, y_train)\n"
      ],
      "metadata": {
        "id": "lqNuE_sRdC24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate GridSearchCV"
      ],
      "metadata": {
        "id": "mbbbscAKdFSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "bnIQsVk9dFjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make our predictions\n"
      ],
      "metadata": {
        "id": "gZnl8ZGOdGLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_svc_model = grid_search.best_estimator_\n",
        "valid_preds_best_svc_model = best_svc_model.predict(X_valid_tfidf)\n",
        "\n",
        "from joblib import dump\n",
        "\n",
        "# save the model\n",
        "dump(best_svc_model, 'best_svc_model.joblib')\n"
      ],
      "metadata": {
        "id": "yY5IvknrdGu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the performance\n"
      ],
      "metadata": {
        "id": "jVC6cHHTdIAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc_best_model_f1 = f1_score(y_valid, valid_preds_best_svc_model, average='weighted')\n",
        "print('Optimized SVC - F1 Score: Validation', svc_best_model_f1)\n"
      ],
      "metadata": {
        "id": "bmsiMm73dIel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this, we can observe tat we have an improvement from 0.90 to approx 0.96 in F1 Score. This suggests that our SVM model is now performing much better in terms of both precision and recall."
      ],
      "metadata": {
        "id": "mBtogYHNgJSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep learning Model**\n",
        "\n",
        "For this, we shall experiment by adding more layers and changing the type of the layers."
      ],
      "metadata": {
        "id": "gr9nW6O5g82h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Calculate the weights for each class\n",
        "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "# Create a dictionary mapping each class to its weight\n",
        "class_weights = dict(enumerate(weights))"
      ],
      "metadata": {
        "id": "ph0nmsV9kqER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# create an encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# fit and transform y_train with the encoder\n",
        "y_train = le.fit_transform(y_train)\n",
        "\n",
        "# transform y_valid with the encoder\n",
        "y_valid = le.transform(y_valid)\n"
      ],
      "metadata": {
        "id": "O4K7AWfCjmgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "import numpy as np\n",
        "\n",
        "y_train = y_train.astype(float)\n",
        "y_valid = y_valid.astype(float)\n",
        "\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df_train['headline'] + ' ' + df_train['short_description'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(df_train['headline'] + ' ' + df_train['short_description'])\n",
        "X_valid = tokenizer.texts_to_sequences(df_valid['headline'] + ' ' + df_valid['short_description'])\n",
        "\n",
        "# Pad the sequences\n",
        "X_train = pad_sequences(X_train, maxlen=100)\n",
        "X_valid = pad_sequences(X_valid, maxlen=100)\n",
        "\n",
        "X = np.concatenate((X_train, X_valid))\n",
        "y = np.concatenate((y_train, y_valid))\n",
        "\n",
        "num_words = 5000\n",
        "embedding_dim = 100\n",
        "max_length = 100\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_valid, y_valid), class_weight=class_weights)\n",
        "\n",
        "# Save the model\n",
        "model.save('text_classifier_DL2.h5')\n"
      ],
      "metadata": {
        "id": "pHfZ-3kTiqP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds_dl_model = (model.predict(X_train) > 0.5).astype(\"int32\")\n",
        "valid_preds_dl_model = (model.predict(X_valid) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "id": "NfTZdKREnAoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_train_dl_model = f1_score(y_train, train_preds_dl_model)\n",
        "f1_valid_dl_model = f1_score(y_valid, valid_preds_dl_model)"
      ],
      "metadata": {
        "id": "VMBQj4kqnO4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Deep Learning Model - F1 Score: Train {f1_train_dl_model}, Validation {f1_valid_dl_model}')"
      ],
      "metadata": {
        "id": "JPjSFoWDnYes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that we have improved from 0.9145728643216079 to 0.9182879377431907 on our validation data."
      ],
      "metadata": {
        "id": "mpEsyoo1n0Cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Validation**"
      ],
      "metadata": {
        "id": "YUHn2KVjoL7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying cross-validation for our Random forest and SVM model"
      ],
      "metadata": {
        "id": "VOwXh3C-oLt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "\n",
        "# Perform cross validation\n",
        "rf_scores = cross_val_score(best_rf_model, X, y, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "print(f'Random Forest cross-validation F1 score: {rf_scores.mean()}')\n",
        "\n"
      ],
      "metadata": {
        "id": "siTzgxdQpVBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_preds_dl_model = (model.predict(X) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "id": "Qpif_8lkrSnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_cv_dl_model = f1_score(y, cv_preds_dl_model)"
      ],
      "metadata": {
        "id": "PcnigqI7uzkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Deep Learning Model - F1 Score: Cross Validation {f1_cv_dl_model}')"
      ],
      "metadata": {
        "id": "fXgPq2zEu6-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Cross Validation has a score of 0.98.\n",
        "Thus making this our best model."
      ],
      "metadata": {
        "id": "2vJVSQtWvCtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shall now use this on test data"
      ],
      "metadata": {
        "id": "2FDmuKZsvhyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_dl_model = (model.predict(X_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "id": "2bd4CIsRvktd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_test_dl_model = f1_score(y_test, test_preds_dl_model)\n",
        "print(f'Deep Learning Model - F1 Score: Test Data {f1_test_dl_model}')"
      ],
      "metadata": {
        "id": "4bW1I2kYvyzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our test results are slightly reduced compared to our cross validation data."
      ],
      "metadata": {
        "id": "2KxHiQeKwBO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train this data on both training and validation data now\n"
      ],
      "metadata": {
        "id": "_s043sQLwZSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, batch_size=32, epochs=5, validation_data=(X_valid, y_valid), class_weight=class_weights)\n",
        "\n",
        "# Save the model\n",
        "model.save('text_classifier_DL3.h5')"
      ],
      "metadata": {
        "id": "0uZhILCvwdfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now apply it to test set"
      ],
      "metadata": {
        "id": "_Sz2oAXZxT1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_dl_model = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "f1_test_dl_model = f1_score(y_test, test_preds_dl_model)\n",
        "print(f'Deep Learning Model - F1 Score: Test Data {f1_test_dl_model}')"
      ],
      "metadata": {
        "id": "Dr4EKxgwxXlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model with more data has given us a slight increase in our score from 0.917 to 0.918"
      ],
      "metadata": {
        "id": "qvAzLIQ1xe1q"
      }
    }
  ]
}